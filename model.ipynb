{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indeed = pd.read_csv(\"data/indeed_data_scientist.csv\")\n",
    "df_linkedin = pd.read_csv(\"data/linkedin_data_scientist.csv\")\n",
    "df = pd.concat([df_indeed, df_linkedin], axis=0)\n",
    "df.drop_duplicates(subset=[\"title\", \"company\", \"text\"], keep=\"first\", inplace=True)\n",
    "df.dropna(subset=[\"text\"], inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indeed_list = pd.read_csv(\"data/indeed_data_scientist_list.csv\")\n",
    "df_linkedin_list = pd.read_csv(\"data/linkedin_data_scientist_list.csv\")\n",
    "df_list = pd.concat([df_indeed_list, df_linkedin_list], axis=0)\n",
    "df_list.drop_duplicates(subset=[\"title\", \"company\", \"text\"], keep=\"first\", inplace=True)\n",
    "df_list.dropna(subset=[\"text\"], inplace=True)\n",
    "df_list.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation removal\n",
    "import string\n",
    "string.punctuation\n",
    "data = pd.DataFrame()\n",
    "\n",
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "data[\"body_text_clean\"] = df[\"text\"].apply(lambda x: remove_punct(x))\n",
    "\n",
    "#  tokenization\n",
    "def tokenize(text):\n",
    "    tokens = re.split(\"\\W+\", text)\n",
    "    return tokens\n",
    "\n",
    "data[\"body_text_tokenized\"] = data[\"body_text_clean\"].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "# stopword removal\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "data[\"body_text_nostop\"] = data[\"body_text_tokenized\"].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# stemming\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data[\"body_text_stemmed\"] = data[\"body_text_nostop\"].apply(lambda x: stemming(x))\n",
    "\n",
    "#  lemmatize\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data[\"body_text_lemmatized\"] = data[\"body_text_nostop\"].apply(lambda x: lemmatizing(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "#     text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    text = [wn.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english', ngram_range=(1,4), strip_accents=\"unicode\", max_features=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"OBAMA\", None, nlp(\"Barack Obama\"))\n",
    "doc = nlp(\"Barack Obama lifts America one last time in emotional farewell\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7732777389095836264, 0, 2)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c84009412823>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_trf_xlnetbasecased_lg\"\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# load model package \"en_core_web_sm\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# nlp = spacy.load(\"en_core_web_sm\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# nlp = spacy.load(\"/path/to/en_core_web_sm\")  # load package from a directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# nlp = spacy.load(\"en\")                       # load model with shortcut link \"en\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;34m\"\"\"Load a model from an installed package.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/en_trf_xlnetbasecased_lg/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, **overrides)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath2str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, exclude, disable)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;31m# Convert to list here in case exclude is (default) tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;31m# Split to support file names like meta.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m             \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p, proc)\u001b[0m\n\u001b[1;32m    933\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m             deserializers[name] = lambda p, proc=proc: proc.from_disk(\n\u001b[0;32m--> 935\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m             )\n\u001b[1;32m    937\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"vocab\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Pipe.from_disk\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;31m# Split to support file names like meta.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m             \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Pipe.from_disk.load_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36mfrom_bytes\u001b[0;34m(self, bytes_data)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrsly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsgpack_loads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mqueue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/srsly/_msgpack_api.py\u001b[0m in \u001b[0;36mmsgpack_loads\u001b[0;34m(data, use_list)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# msgpack-python docs suggest disabling gc before unpacking large messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsgpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/srsly/msgpack/__init__.py\u001b[0m in \u001b[0;36munpackb\u001b[0;34m(packed, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mobject_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object_hook'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object_hook'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decode_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_unpackb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_unpacker.pyx\u001b[0m in \u001b[0;36msrsly.msgpack._unpacker.unpackb\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_trf_xlnetbasecased_lg\")           # load model package \"en_core_web_sm\"\n",
    "# nlp = spacy.load(\"en_core_web_sm\") \n",
    "# nlp = spacy.load(\"/path/to/en_core_web_sm\")  # load package from a directory\n",
    "# nlp = spacy.load(\"en\")                       # load model with shortcut link \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Larry Page', 'PERSON'), ('Google', 'ORG')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Larry Page founded Google\")\n",
    "# Text and label of named entity span\n",
    "[(ent.text, ent.label_) for ent in doc.ents]\n",
    "# [('Larry Page', 'PERSON'), ('Google', 'ORG')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7732777389095836264, 1, 3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "matcher.add(\"OBAMA\", None, nlp(\"Obama lifts\"))\n",
    "doc = nlp(\"Barack Obama lifts America one last time in emotional farewell\")\n",
    "matches = matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7342852\n",
      "0.4336572\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import numpy\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "is_using_gpu = spacy.prefer_gpu()\n",
    "if is_using_gpu:\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "\n",
    "nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
    "doc = nlp(\"Here is some text to encode.\")\n",
    "assert doc.tensor.shape == (7, 768)  # Always has one row per token\n",
    "doc._.trf_word_pieces_  # String values of the wordpieces\n",
    "doc._.trf_word_pieces  # Wordpiece IDs (note: *not* spaCy's hash values!)\n",
    "doc._.trf_alignment  # Alignment between spaCy tokens and wordpieces\n",
    "# The raw transformer output has one row per wordpiece.\n",
    "assert len(doc._.trf_last_hidden_state) == len(doc._.trf_word_pieces)\n",
    "# To avoid losing information, we calculate the doc.tensor attribute such that\n",
    "# the sum-pooled vectors match (apart from numeric error)\n",
    "assert_almost_equal(doc.tensor.sum(axis=0), doc._.trf_last_hidden_state.sum(axis=0), decimal=5)\n",
    "span = doc[2:4]\n",
    "# Access the tensor from Span elements (especially helpful for sentences)\n",
    "assert numpy.array_equal(span.tensor, doc.tensor[2:4])\n",
    "# .vector and .similarity use the transformer outputs\n",
    "apple1 = nlp(\"Apple shares rose on the news.\")\n",
    "apple2 = nlp(\"Apple sold fewer iPhones this quarter.\")\n",
    "apple3 = nlp(\"Apple pie is delicious.\")\n",
    "print(apple1[0].similarity(apple2[0]))  # 0.73428553\n",
    "print(apple1[0].similarity(apple3[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9746173329890798"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_trf_xlnetbasecased_lg\") \n",
    "\n",
    "doc1 = nlp(\"How do I turn sound on/off?\")\n",
    "doc2 = nlp(\"How do I obtain a pet?\")\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_trf_xlnetbasecased_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentencizer', 'trf_wordpiecer', 'trf_tok2vec']\n",
      "0 {'trf_textcat': 0.5}\n",
      "1 {'trf_textcat': 0.3553078770637512}\n",
      "2 {'trf_textcat': 0.2451152205467224}\n",
      "3 {'trf_textcat': 0.05905628576874733}\n",
      "4 {'trf_textcat': 0.014026578515768051}\n",
      "5 {'trf_textcat': 0.002765323268249631}\n",
      "6 {'trf_textcat': 0.0006972913397476077}\n",
      "7 {'trf_textcat': 0.0001908408012241125}\n",
      "8 {'trf_textcat': 3.530673711793497e-05}\n",
      "9 {'trf_textcat': 1.5150550098042004e-05}\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = [\n",
    "    (\"text1\", {\"cats\": {\"POSITIVE\": 1.0, \"NEGATIVE\": 0.0}})\n",
    "]\n",
    "\n",
    "import spacy\n",
    "from spacy.util import minibatch\n",
    "import random\n",
    "import torch\n",
    "\n",
    "is_using_gpu = spacy.prefer_gpu()\n",
    "if is_using_gpu:\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "\n",
    "nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
    "print(nlp.pipe_names) # [\"sentencizer\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "textcat = nlp.create_pipe(\"trf_textcat\", config={\"exclusive_classes\": True})\n",
    "for label in (\"POSITIVE\", \"NEGATIVE\"):\n",
    "    textcat.add_label(label)\n",
    "nlp.add_pipe(textcat)\n",
    "\n",
    "optimizer = nlp.resume_training()\n",
    "for i in range(10):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    for batch in minibatch(TRAIN_DATA, size=8):\n",
    "        texts, cats = zip(*batch)\n",
    "        nlp.update(texts, cats, sgd=optimizer, losses=losses)\n",
    "    print(i, losses)\n",
    "nlp.to_disk(\"/bert-textcat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Designs and executes innovative and robust data solutions in collaboration with internal stakeholders to support team objectives. Areas of responsibility include data architecture and infrastructure, scheduled data retrievals, data processing, scalability, dealing with various internal and external data under both big data and traditional data platforms.     Conducts preliminary analyses on data involving basic statistical approaches and provides solutions to treat various data deficiencies to support data integrity.     Leads and conducts industry, demographic, and other consumer research to understand new data sources and applications of analytics.     Focuses on innovation and process improvement to promote leading practice and efficiency across the organization.     Builds predictive models and end-to-end automated solutions for insurance cost, customer demand and other more complex problems.     Presents sophisticated statistical and machine learning models to various stakeholders and provide training and coaching.     University degree in a quantitative field (e.g. Actuarial Sciences, Mathematics, Statistics, Computer Science)     Good understanding of statistical methods     Excellent communication and organizational skills     3+ years of SAS or SQL programming experience     3+ years of R or Python programming experience     Knowledge of Emblem or Radar an asset     Knowledge of modeling techniques such as GLM, GAM, XGBoost and other is desirable     Knowledge of Git and Bitbucket an asset   We also take potential into consideration. If you don’t have this exact experience, but you know you have what it takes, be sure to give us more insight through your application and cover letter.   Go ahead and expect a lot — you deserve it.   We offer:     Competitive salaries, with potential for an annual raise and bonus     Pension and savings programs, with company-matched RRSP contributions     Generous time away, including vacation and personal needs days     Paid volunteer days and company matching on charitable donations     Educational resources, tuition assistance, and paid time off to study for exams     Two annual wellness campaigns — participants earn up to $300 each year to spend on almost anything supporting health and work-life balance (think things like spa days, daycare, pet grooming)     An unlimited employee referral bonus program     Flexible work schedule     Discounts on products and services  \n",
      "\n",
      "\n",
      "Element AI is one of the world's largest artificial intelligence companies, with a shared goal of using our extensive research to develop AI products and software that will solve some of the most challenging issues facing businesses and society as a whole.   We are founded on the belief that humans should be at the heart of everything we do, and have adopted a people-first and collaborative mindset to ensure we collectively drive real and ethically-sound results.   As an applied research scientist in the Financial Products group at Element AI, you will sit in a cross-functional team with several other people with a diversity of skill sets and you will enjoy a unique opportunity to use your creativity towards applying cutting edge machine learning and operations research methods to problems with significant impact in the financial industry. While doing so you will have access to the best hardware and you will be assisted by a team of IT experts and software developers, to ensure most of your time is spent on core R&D challenges.   What you'll do   You will be confronted with real-world challenges and datasets, and you will need to use your AI/ML expertise and creativity to apply existing methods and develop new ones to solve these problems in a practical and scalable way. You will be called upon to assess possibilities and provide ideas and input into product design choices. This will involve understanding your customer (internal or external) needs and and translating them into solutions that address those needs.   You will be expected to both do the necessary research to propose appropriate models/techniques and to have the necessary expertise to implement and train the models yourself. Where useful, you will not hesitate to employ classical machine learning methods, but you are enthusiastic at the idea of pushing the boundaries of deep learning and AI.   Finally, you will be expected to embrace the fact that the value of your work is ultimately reflected in the impact it has on the end-customers using our products and to find ways of measuring that impact as an integral part of your mission.   What we're looking for   You have significant understanding of the underlying theory of machine learning, operations research or related AI field. In the Financial Products group, we are specifically interested in skill sets related to reinforcement learning, dynamic programming, deep learning, time series analysis & forecasting, optimization, simulation, generative models, and similar. This expertise can come from extensive studies, previous industrial experience or awesome self-taught projects you have done on a personal basis. You're also a solid programmer and you're comfortable doing scientific programming as well as product development and do not mind getting your hands dirty in various coding and engineering tasks. This includes embracing modern devops principles to development and working in a setting where code is expected to be shared and peer-reviewed.   You understand that running a model on the varied and often noisy data that arises in a commercial context differs significantly from running it on a clean academic dataset, and that modifying a model or technique to work in that setting can be a significant and sometimes frustrating challenge. You embrace this challenge and may even have previous experience tackling it.   You learn autonomously and will enthusiastically stay up to date in the literature and techniques of your field while participating in the various learning opportunities we offer.    What we offer for your valuable work:     Work closely with other AI enthusiasts;   Enjoy your work and life harmony;   Leave your mark in a thriving industry;   Apply your talent to tackling new challenges everyday;   Be surprised at how much you will learn;   Open and inclusive company culture.    We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9944838442872223"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_trf_xlnetbasecased_lg\") \n",
    "\n",
    "doc1 = nlp(df_list[\"text\"].loc[20])\n",
    "doc2 = nlp(df[\"text\"].loc[69])\n",
    "print(doc1)\n",
    "print(\"\\n\")\n",
    "print(doc2)\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Designs True 154.30315 True\n",
      "and True 67.354355 True\n",
      "executes True 212.7289 True\n",
      "innovative True 111.707405 True\n",
      "and True 100.290184 True\n",
      "robust True 98.7646 True\n",
      "data True 122.82915 True\n",
      "solutions True 104.98659 True\n",
      "in True 141.44861 True\n",
      "collaboration True 165.60202 True\n",
      "with True 146.56181 True\n",
      "internal True 120.14131 True\n",
      "stakeholders True 123.93559 True\n",
      "to True 96.71213 True\n",
      "support True 113.800476 True\n",
      "team True 113.8621 True\n",
      "objectives True 100.63677 True\n",
      ". True 81.189644 True\n",
      "Areas True 178.37143 True\n",
      "of True 106.5334 True\n",
      "responsibility True 78.63831 True\n",
      "include True 118.340576 True\n",
      "data True 109.61223 True\n",
      "architecture True 98.928085 True\n",
      "and True 61.97479 True\n",
      "infrastructure True 78.80913 True\n",
      ", True 56.109295 True\n",
      "scheduled True 77.667755 True\n",
      "data True 104.41756 True\n",
      "retrievals True 143.9374 True\n",
      ", True 53.475502 True\n",
      "data True 83.33658 True\n",
      "processing True 55.359287 True\n",
      ", True 145.27928 True\n",
      "scalability True 334.43793 True\n",
      ", True 124.145874 True\n",
      "dealing True 104.29879 True\n",
      "with True 109.56873 True\n",
      "various True 118.12135 True\n",
      "internal True 112.20444 True\n",
      "and True 86.52496 True\n",
      "external True 109.374886 True\n",
      "data True 104.75965 True\n",
      "under True 123.119125 True\n",
      "both True 156.28127 True\n",
      "big True 92.74106 True\n",
      "data True 115.89209 True\n",
      "and True 73.530045 True\n",
      "traditional True 91.474495 True\n",
      "data True 112.09926 True\n",
      "platforms True 81.684296 True\n",
      ". True 80.01537 True\n",
      "     True 6.9963818 True\n",
      "Conducts True 241.5534 True\n",
      "preliminary True 135.54277 True\n",
      "analyses True 121.27858 True\n",
      "on True 90.32149 True\n",
      "data True 124.89497 True\n",
      "involving True 117.26826 True\n",
      "basic True 105.245186 True\n",
      "statistical True 122.255325 True\n",
      "approaches True 103.44004 True\n",
      "and True 90.73873 True\n",
      "provides True 128.40237 True\n",
      "solutions True 93.11827 True\n",
      "to True 91.28287 True\n",
      "treat True 119.870026 True\n",
      "various True 133.113 True\n",
      "data True 121.792114 True\n",
      "deficiencies True 96.56969 True\n",
      "to True 63.52411 True\n",
      "support True 90.181564 True\n",
      "data True 101.5492 True\n",
      "integrity True 81.87421 True\n",
      ". True 83.19833 True\n",
      "     True 4.7703013 True\n",
      "Leads True 110.27495 True\n",
      "and True 48.349323 True\n",
      "conducts True 175.53864 True\n",
      "industry True 68.77615 True\n",
      ", True 54.793896 True\n",
      "demographic True 81.55441 True\n",
      ", True 77.79328 True\n",
      "and True 92.133705 True\n",
      "other True 89.85181 True\n",
      "consumer True 85.694595 True\n",
      "research True 106.47098 True\n",
      "to True 84.71966 True\n",
      "understand True 105.82885 True\n",
      "new True 98.45271 True\n",
      "data True 104.07124 True\n",
      "sources True 86.08771 True\n",
      "and True 76.209175 True\n",
      "applications True 92.44755 True\n",
      "of True 72.26023 True\n",
      "analytics True 98.46534 True\n",
      ". True 72.28184 True\n",
      "     True 9.021648 True\n",
      "Focuses True 227.37349 True\n",
      "on True 111.933205 True\n",
      "innovation True 92.57278 True\n",
      "and True 68.80215 True\n",
      "process True 110.02961 True\n",
      "improvement True 99.64737 True\n",
      "to True 107.00667 True\n",
      "promote True 93.49781 True\n",
      "leading True 79.88573 True\n",
      "practice True 103.55118 True\n",
      "and True 104.52682 True\n",
      "efficiency True 104.68879 True\n",
      "across True 153.84193 True\n",
      "the True 162.9048 True\n",
      "organization True 106.68573 True\n",
      ". True 87.49153 True\n",
      "     True 3.729541 True\n",
      "Builds True 193.8455 True\n",
      "predictive True 183.72493 True\n",
      "models True 89.986 True\n",
      "and True 72.50266 True\n",
      "end True 161.88411 True\n",
      "- True 128.49889 True\n",
      "to True 196.06001 True\n",
      "- True 157.27922 True\n",
      "end True 152.28386 True\n",
      "automated True 87.38332 True\n",
      "solutions True 72.115654 True\n",
      "for True 70.036 True\n",
      "insurance True 65.496086 True\n",
      "cost True 77.4687 True\n",
      ", True 74.503136 True\n",
      "customer True 77.44422 True\n",
      "demand True 94.41402 True\n",
      "and True 82.343185 True\n",
      "other True 96.83319 True\n",
      "more True 103.665504 True\n",
      "complex True 68.125435 True\n",
      "problems True 79.70531 True\n",
      ". True 79.29288 True\n",
      "     True 5.828822 True\n",
      "Presents True 168.31494 True\n",
      "sophisticated True 88.01343 True\n",
      "statistical True 77.17909 True\n",
      "and True 101.26314 True\n",
      "machine True 112.35095 True\n",
      "learning True 107.688126 True\n",
      "models True 103.73829 True\n",
      "to True 152.0636 True\n",
      "various True 110.755936 True\n",
      "stakeholders True 99.9302 True\n",
      "and True 118.74584 True\n",
      "provide True 98.50774 True\n",
      "training True 101.213875 True\n",
      "and True 70.90739 True\n",
      "coaching True 73.752426 True\n",
      ". True 62.42398 True\n",
      "     True 0.6684332 True\n",
      "University True 106.639084 True\n",
      "degree True 120.07046 True\n",
      "in True 127.57247 True\n",
      "a True 126.31006 True\n",
      "quantitative True 117.90703 True\n",
      "field True 190.13124 True\n",
      "( True 257.27426 True\n",
      "e.g. True 833.72876 True\n",
      "Actuarial True 328.13953 True\n",
      "Sciences True 124.21502 True\n",
      ", True 126.16541 True\n",
      "Mathematics True 108.46777 True\n",
      ", True 99.340775 True\n",
      "Statistics True 101.67413 True\n",
      ", True 132.60814 True\n",
      "Computer True 111.99042 True\n",
      "Science True 101.89565 True\n",
      ") True 200.36769 True\n",
      "     True 0.6684332 True\n",
      "Good True 119.05428 True\n",
      "understanding True 94.6022 True\n",
      "of True 67.47521 True\n",
      "statistical True 109.406235 True\n",
      "methods True 170.77821 True\n",
      "     True 0.6684332 True\n",
      "Excellent True 204.5208 True\n",
      "communication True 115.33598 True\n",
      "and True 105.710144 True\n",
      "organizational True 122.25179 True\n",
      "skills True 121.74125 True\n",
      "     True 0.6684332 True\n",
      "3 True 173.85123 True\n",
      "+ True 162.74034 True\n",
      "years True 190.45595 True\n",
      "of True 159.39796 True\n",
      "SAS True 246.87456 True\n",
      "or True 223.79579 True\n",
      "SQL True 141.63875 True\n",
      "programming True 111.89381 True\n",
      "experience True 147.81929 True\n",
      "     True 0.6684332 True\n",
      "3 True 183.76218 True\n",
      "+ True 165.58249 True\n",
      "years True 208.47993 True\n",
      "of True 155.41385 True\n",
      "R True 97.35219 True\n",
      "or True 121.838745 True\n",
      "Python True 102.1334 True\n",
      "programming True 98.255486 True\n",
      "experience True 139.01463 True\n",
      "     True 0.6684332 True\n",
      "Knowledge True 124.72137 True\n",
      "of True 119.86745 True\n",
      "Emblem True 153.06624 True\n",
      "or True 105.981705 True\n",
      "Radar True 189.8122 True\n",
      "an True 217.85976 True\n",
      "asset True 154.48074 True\n",
      "     True 0.6684332 True\n",
      "Knowledge True 119.90452 True\n",
      "of True 135.4034 True\n",
      "modeling True 75.0227 True\n",
      "techniques True 73.40878 True\n",
      "such True 94.411674 True\n",
      "as True 64.033195 True\n",
      "GLM True 111.03531 True\n",
      ", True 102.738045 True\n",
      "GAM True 76.679016 True\n",
      ", True 82.68007 True\n",
      "XGBoost True 366.61255 True\n",
      "and True 82.21677 True\n",
      "other True 131.77841 True\n",
      "is True 175.56651 True\n",
      "desirable True 97.85284 True\n",
      "     True 0.6684332 True\n",
      "Knowledge True 125.775894 True\n",
      "of True 193.80353 True\n",
      "Git True 185.42337 True\n",
      "and True 141.71497 True\n",
      "Bitbucket True 346.96234 True\n",
      "an True 241.40292 True\n",
      "asset True 138.05688 True\n",
      "   True 0.6684332 True\n",
      "We True 150.44077 True\n",
      "also True 144.54758 True\n",
      "take True 102.913536 True\n",
      "potential True 94.55325 True\n",
      "into True 136.3143 True\n",
      "consideration True 112.37196 True\n",
      ". True 80.80941 True\n",
      "If True 141.21898 True\n",
      "you True 160.564 True\n",
      "do True 290.3425 True\n",
      "n’t True 209.42693 True\n",
      "have True 151.78644 True\n",
      "this True 150.66881 True\n",
      "exact True 116.238144 True\n",
      "experience True 104.888885 True\n",
      ", True 119.03611 True\n",
      "but True 148.37305 True\n",
      "you True 143.25487 True\n",
      "know True 119.26355 True\n",
      "you True 181.2203 True\n",
      "have True 176.17041 True\n",
      "what True 125.18722 True\n",
      "it True 123.76992 True\n",
      "takes True 147.89919 True\n",
      ", True 116.05327 True\n",
      "be True 132.5811 True\n",
      "sure True 134.1183 True\n",
      "to True 140.3466 True\n",
      "give True 133.11317 True\n",
      "us True 177.98137 True\n",
      "more True 134.0486 True\n",
      "insight True 114.70639 True\n",
      "through True 147.46216 True\n",
      "your True 158.7432 True\n",
      "application True 160.01184 True\n",
      "and True 171.98564 True\n",
      "cover True 126.33487 True\n",
      "letter True 181.6714 True\n",
      ". True 95.22593 True\n",
      "   True 13.093545 True\n",
      "Go True 127.528915 True\n",
      "ahead True 120.02404 True\n",
      "and True 128.86777 True\n",
      "expect True 119.01539 True\n",
      "a True 141.62769 True\n",
      "lot True 321.96228 True\n",
      "— True 13.093545 True\n",
      "you True 372.58154 True\n",
      "deserve True 133.53389 True\n",
      "it True 180.21413 True\n",
      ". True 109.786835 True\n",
      "   True 0.7808889 True\n",
      "We True 73.83931 True\n",
      "offer True 97.28873 True\n",
      ": True 179.86105 True\n",
      "     True 0.7808889 True\n",
      "Competitive True 52.675667 True\n",
      "salaries True 61.974346 True\n",
      ", True 141.79948 True\n",
      "with True 145.68811 True\n",
      "potential True 84.436325 True\n",
      "for True 137.43771 True\n",
      "an True 138.5964 True\n",
      "annual True 104.7727 True\n",
      "raise True 95.06392 True\n",
      "and True 120.11176 True\n",
      "bonus True 96.92772 True\n",
      "     True 0.7808889 True\n",
      "Pension True 61.90739 True\n",
      "and True 77.83841 True\n",
      "savings True 73.70657 True\n",
      "programs True 104.32207 True\n",
      ", True 129.43861 True\n",
      "with True 123.635544 True\n",
      "company True 82.64091 True\n",
      "- True 97.43372 True\n",
      "matched True 246.28535 True\n",
      "RRSP True 182.76753 True\n",
      "contributions True 91.16153 True\n",
      "     True 0.7808889 True\n",
      "Generous True 103.39707 True\n",
      "time True 101.66826 True\n",
      "away True 130.33702 True\n",
      ", True 112.48288 True\n",
      "including True 130.62694 True\n",
      "vacation True 97.88547 True\n",
      "and True 74.623604 True\n",
      "personal True 100.13894 True\n",
      "needs True 121.082596 True\n",
      "days True 69.415215 True\n",
      "     True 0.7808889 True\n",
      "Paid True 106.51825 True\n",
      "volunteer True 77.51343 True\n",
      "days True 100.875984 True\n",
      "and True 122.61734 True\n",
      "company True 87.771164 True\n",
      "matching True 143.10596 True\n",
      "on True 88.61914 True\n",
      "charitable True 71.86748 True\n",
      "donations True 72.74748 True\n",
      "     True 0.7808889 True\n",
      "Educational True 42.23846 True\n",
      "resources True 63.813026 True\n",
      ", True 50.724205 True\n",
      "tuition True 57.86676 True\n",
      "assistance True 61.80422 True\n",
      ", True 51.240803 True\n",
      "and True 74.489815 True\n",
      "paid True 85.60912 True\n",
      "time True 100.34477 True\n",
      "off True 97.39231 True\n",
      "to True 57.468967 True\n",
      "study True 64.7825 True\n",
      "for True 48.81665 True\n",
      "exams True 74.818954 True\n",
      "     True 0.7808889 True\n",
      "Two True 62.891262 True\n",
      "annual True 63.604362 True\n",
      "wellness True 194.67137 True\n",
      "campaigns True 263.95 True\n",
      "— True 0.7808889 True\n",
      "participants True 286.0514 True\n",
      "earn True 112.51482 True\n",
      "up True 153.94965 True\n",
      "to True 135.49261 True\n",
      "$ True 30.853382 True\n",
      "300 True 30.853382 True\n",
      "each True 143.09117 True\n",
      "year True 105.46803 True\n",
      "to True 98.30285 True\n",
      "spend True 87.166855 True\n",
      "on True 103.587204 True\n",
      "almost True 140.5732 True\n",
      "anything True 118.7206 True\n",
      "supporting True 93.00225 True\n",
      "health True 109.538124 True\n",
      "and True 96.27376 True\n",
      "work True 84.747215 True\n",
      "- True 128.30754 True\n",
      "life True 70.60625 True\n",
      "balance True 133.78783 True\n",
      "( True 253.21123 True\n",
      "think True 124.16652 True\n",
      "things True 143.73114 True\n",
      "like True 84.71472 True\n",
      "spa True 101.89709 True\n",
      "days True 109.587524 True\n",
      ", True 60.66891 True\n",
      "daycare True 169.44958 True\n",
      ", True 100.678024 True\n",
      "pet True 103.8423 True\n",
      "grooming True 279.93906 True\n",
      ") True 227.54912 True\n",
      "     True 0.7808889 True\n",
      "An True 96.07712 True\n",
      "unlimited True 64.275085 True\n",
      "employee True 69.76611 True\n",
      "referral True 79.32377 True\n",
      "bonus True 71.928276 True\n",
      "program True 92.59794 True\n",
      "     True 0.7808889 True\n",
      "Flexible True 177.68587 True\n",
      "work True 38.261288 True\n",
      "schedule True 48.183804 True\n",
      "     True 0.7808889 True\n",
      "Discounts True 129.27809 True\n",
      "on True 70.57939 True\n",
      "products True 64.406296 True\n",
      "and True 52.090984 True\n",
      "services True 72.002594 True\n",
      "  True 0.7808889 True\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Designs and executes innovative and robust data solutions in collaboration with internal stakeholders to support team objectives. Areas of responsibility include data architecture and infrastructure, scheduled data retrievals, data processing, scalability, dealing with various internal and external data under both big data and traditional data platforms.     Conducts preliminary analyses on data involving basic statistical approaches and provides solutions to treat various data deficiencies to support data integrity.     Leads and conducts industry, demographic, and other consumer research to understand new data sources and applications of analytics.     Focuses on innovation and process improvement to promote leading practice and efficiency across the organization.     Builds predictive models and end-to-end automated solutions for insurance cost, customer demand and other more complex problems.     Presents sophisticated statistical and machine learning models to various stakeholders and provide training and coaching.     University degree in a quantitative field (e.g. Actuarial Sciences, Mathematics, Statistics, Computer Science)     Good understanding of statistical methods     Excellent communication and organizational skills     3+ years of SAS or SQL programming experience     3+ years of R or Python programming experience     Knowledge of Emblem or Radar an asset     Knowledge of modeling techniques such as GLM, GAM, XGBoost and other is desirable     Knowledge of Git and Bitbucket an asset   We also take potential into consideration. If you don’t have this exact experience, but you know you have what it takes, be sure to give us more insight through your application and cover letter.   Go ahead and expect a lot — you deserve it.   We offer:     Competitive salaries, with potential for an annual raise and bonus     Pension and savings programs, with company-matched RRSP contributions     Generous time away, including vacation and personal needs days     Paid volunteer days and company matching on charitable donations     Educational resources, tuition assistance, and paid time off to study for exams     Two annual wellness campaigns — participants earn up to $300 each year to spend on almost anything supporting health and work-life balance (think things like spa days, daycare, pet grooming)     An unlimited employee referral bonus program     Flexible work schedule     Discounts on products and services  '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[\"text\"].loc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
