{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gazpacho import get, Soup\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from bs4 import BeautifulSoup             \n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "# import sqlite3\n",
    "# from sqlite3 import Error\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "browser = Firefox(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Indeed Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indeed_scraper(job_title, jobs_per_page, search_radius, pages):\n",
    "    '''\n",
    "    1. Automatically uses \"+\" as a space delimiter when entering job_title\n",
    "    \n",
    "    '''\n",
    "    job_title = job_title.replace(\" \", \"+\")\n",
    "    url = f\"https://ca.indeed.com/jobs?q={job_title}&l=Toronto%2C+ON&limit={jobs_per_page}&radius={search_radius}&start={pages}\"\n",
    "    html = get(url)\n",
    "    soup = Soup(html)\n",
    "    jerbs_index = soup.find(\"td\", {\"id\": \"resultsCol\"})\n",
    "\n",
    "    jerbs_indeed = []\n",
    "    for i in range(len(jerbs_index.find(\"div\", {\"class\": \"title\"}, mode=\"all\"))):\n",
    "        list_1 = {}\n",
    "        list_1[\"title\"] = jerbs_index.find(\"div\", {\"class\": \"title\"}, mode=\"all\")[i].find(\"a\").attrs[\"title\"]\n",
    "        if jerbs_index.find(\"span\", {\"class\": \"company\"}, mode=\"all\")[i].text == \"\":\n",
    "            list_1[\"company\"] = jerbs_index.find(\"div\", {\"class\": \"sjcl\"}, mode=\"all\")[i].find(\"a\")[0].text\n",
    "        else: \n",
    "            list_1[\"company\"] = jerbs_index.find(\"span\", {\"class\": \"company\"}, mode=\"all\")[i].text\n",
    "        list_1[\"url\"] = \"https://ca.indeed.com\"+jerbs_index.find(\"div\", {\"class\": \"title\"}, mode=\"all\")[i].find(\"a\").attrs[\"href\"]\n",
    "        time.sleep(random.randint(1, 10) / 10)\n",
    "        html_jerb = get(list_1[\"url\"])\n",
    "        soup_jerb = Soup(html_jerb)\n",
    "        list_1[\"text\"] = BeautifulSoup(\" \".join([i.html for i in soup_jerb.find(\"div\", {\"class\": \"jobsearch-jobDescriptionText\"}, mode=\"all\")])).get_text(\" \").replace(\"\\n\", \" \") \n",
    "        jerbs_indeed.append(list_1)\n",
    "        time.sleep(random.randint(1, 10) / 10)\n",
    "    return jerbs_indeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape with Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = \"data scientist\"\n",
    "jobs_per_page = 50\n",
    "search_radius = 100\n",
    "# pages = 0\n",
    "pages = [0, 50, 100, 150, 200, 250, 300] # list to iterate through\n",
    "pg = []\n",
    "for page in pages:\n",
    "    scraped = indeed_scraper(job_title, jobs_per_page, search_radius, page)\n",
    "    pg += scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting to csv\n",
    "filename_job = job_title.replace(\" \", \"_\").lower()\n",
    "df_indeed = pd.DataFrame(pg)\n",
    "# clean up duplicates\n",
    "df_indeed.drop_duplicates(subset=[\"title\", \"company\", \"text\"], keep=\"first\", inplace=True)\n",
    "df_indeed.reset_index(inplace=True, drop=True)\n",
    "df_indeed.to_csv(f\"indeed_{filename_job}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## LinkedIn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkedin_scraper(job_title, clicks_linkedin):\n",
    "    job_title = job_title.replace(\" \", \"%20\")\n",
    "    url_linkedin = f\"https://www.linkedin.com/jobs/search?keywords={job_title}&location=Toronto%2C%20Ontario%2C%20Canada&trk=homepage-jobseeker_jobs-search-bar_search-submit&redirect=false&position=1&pageNum=0\"\n",
    "    browser.get(url_linkedin)\n",
    "\n",
    "    for _ in range(clicks_linkedin):\n",
    "    # clicking to get more jobs\n",
    "        browser.find_element_by_class_name(\"see-more-jobs\").click()\n",
    "        browser.implicitly_wait(random.randint(random.randint(10, 15), random.randint(16, 20))) # seconds\n",
    "\n",
    "    html_linkedin = browser.page_source\n",
    "    soup_linkedin = Soup(html_linkedin)\n",
    "    \n",
    "    jerbs_linkedin = []\n",
    "    for i in range(len(soup_linkedin.find(\"li\", {\"class\": \"result-card job-result-card result-card--with-hover-state\"}, mode=\"all\"))):\n",
    "        list_2 = {}\n",
    "        list_2[\"title\"] = soup_linkedin.find(\"li\", {\"class\": \"result-card job-result-card result-card--with-hover-state\"}, mode=\"all\")[i].find(\"h3\", {\"class\":\"result-card__title job-result-card__title\"}).text\n",
    "        list_2[\"company\"] = soup_linkedin.find(\"li\", {\"class\": \"result-card job-result-card result-card--with-hover-state\"}, mode=\"all\")[i].find(\"h4\", {\"class\":\"result-card__subtitle job-result-card__subtitle\"}).text\n",
    "        list_2[\"url\"] = soup_linkedin.find(\"li\", {\"class\": \"result-card job-result-card result-card--with-hover-state\"}, mode=\"all\")[i].find(\"a\", {\"class\": \"result-card__full-card-link\"}, mode=\"all\")[0].attrs[\"href\"]\n",
    "        browser.implicitly_wait(random.randint(random.randint(1, 5), random.randint(6, 10))) # seconds\n",
    "#         time.sleep(random.randint(1, 10) / 8)\n",
    "        html_jerb = get(list_2[\"url\"])\n",
    "        soup_jerb = Soup(html_jerb)\n",
    "        list_2[\"text\"] = BeautifulSoup(\" \".join([i.html for i in soup_jerb.find(\"div\", {\"class\": \"description__text description__text--rich\"}, mode=\"all\")])).get_text(\" \").replace(\"\\n\", \" \")\n",
    "        jerbs_linkedin.append(list_2)\n",
    "        browser.implicitly_wait(random.randint(random.randint(1, 5), random.randint(6, 10))) # seconds\n",
    "#         time.sleep(random.randint(1, 10) / 8)\n",
    "    return jerbs_linkedin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape with Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_linkedin = 20\n",
    "job_title = \"Data Scientist\"\n",
    "\n",
    "linkedin_jerbs = linkedin_scraper(job_title, clicks_linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting to csv\n",
    "filename_job = job_title.replace(\" \", \"_\").lower()\n",
    "df_linkedin = pd.DataFrame(linkedin_jerbs)\n",
    "df_linkedin.drop_duplicates(subset=[\"title\", \"company\", \"text\"], keep=\"first\", inplace=True)\n",
    "df_linkedin.reset_index(inplace=True, drop=True)\n",
    "df_linkedin.to_csv(f\"linkedin_{filename_job}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
